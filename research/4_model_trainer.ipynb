{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/azureuser/Text_Summarizer_Project/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/azureuser/Text_Summarizer_Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../.\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir : Path\n",
    "    data_path: Path\n",
    "    model_ckpt : Path\n",
    "    num_train_epochs : int\n",
    "    warmup_steps : int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size : int\n",
    "    weight_decay : float\n",
    "    logging_steps : int\n",
    "    evaluation_strategy : str\n",
    "    eval_steps : int\n",
    "    save_steps : float\n",
    "    gradient_accumulation_steps : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textsummarizer.constant import *\n",
    "from textsummarizer.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAM_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArgumants\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = params.per_device_eval_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.eval_steps,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "            \n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-22 12:20:17,344: INFO: config: PyTorch version 2.1.2+rocm5.6 available.]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "from datasets import load_dataset,load_from_disk,load_metric\n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "\n",
    "        dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "        \n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir = self.config.root_dir,\n",
    "            num_train_epochs = self.config.num_train_epochs,\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = self.config.per_device_eval_batch_size,\n",
    "            weight_decay = self.config.weight_decay,\n",
    "            logging_steps = self.config.logging_steps,\n",
    "            evaluation_strategy = self.config.evaluation_strategy,\n",
    "            eval_steps = self.config.eval_steps,\n",
    "            save_steps = self.config.save_steps,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps)\n",
    "\n",
    "\n",
    "        # trainer_args = TrainingArguments(\n",
    "        #     output_dir=self.config.root_dir, \n",
    "        #     num_train_epochs= 1, \n",
    "        #     warmup_steps=500,\n",
    "        #     per_device_train_batch_size=1, \n",
    "        #     per_device_eval_batch_size=1,\n",
    "        #     weight_decay=0.01, \n",
    "        #     logging_steps=10,\n",
    "        #     evaluation_strategy='steps', \n",
    "        #     eval_steps=500, \n",
    "        #     save_steps=1e6,\n",
    "        #     gradient_accumulation_steps=16\n",
    "        # ) \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args = trainer_args,\n",
    "            tokenizer = tokenizer,\n",
    "            data_collator = seq2seq_data_collator,\n",
    "            train_dataset = dataset['test'][:10],\n",
    "            eval_dataset = dataset['validation'][:5])\n",
    "        \n",
    "        trainer.train()\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "\n",
    "        dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "        \n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir = self.config.root_dir,\n",
    "            num_train_epochs = self.config.num_train_epochs,\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size = self.config.per_device_eval_batch_size,\n",
    "            weight_decay = self.config.weight_decay,\n",
    "            logging_steps = self.config.logging_steps,\n",
    "            evaluation_strategy = self.config.evaluation_strategy,\n",
    "            eval_steps = self.config.eval_steps,\n",
    "            save_steps = self.config.save_steps,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps)\n",
    "\n",
    "\n",
    "        # trainer_args = TrainingArguments(\n",
    "        #     output_dir=self.config.root_dir, \n",
    "        #     num_train_epochs= 1, \n",
    "        #     warmup_steps=500,\n",
    "        #     per_device_train_batch_size=1, \n",
    "        #     per_device_eval_batch_size=1,\n",
    "        #     weight_decay=0.01, \n",
    "        #     logging_steps=10,\n",
    "        #     evaluation_strategy='steps', \n",
    "        #     eval_steps=500, \n",
    "        #     save_steps=1e6,\n",
    "        #     gradient_accumulation_steps=16\n",
    "        # ) \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args = trainer_args,\n",
    "            tokenizer = tokenizer,\n",
    "            data_collator = seq2seq_data_collator,\n",
    "            train_dataset = dataset['test'],\n",
    "            eval_dataset = dataset['validation'])\n",
    "        \n",
    "        trainer.train()\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-22 12:20:18,969: INFO: common: yaml file:config/config.yaml loaded successfully]\n",
      "[2024-01-22 12:20:18,980: INFO: common: yaml file:params.yaml loaded successfully]\n",
      "[2024-01-22 12:20:18,984: INFO: common: directory:artifacts already exists]\n",
      "[2024-01-22 12:20:18,984: INFO: common: directory:artifacts/model_trainer already exists]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 31:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config = model_trainer_config)\n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002:00:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Vega 10 [Radeon Instinct MI25 MxGPU]\n"
     ]
    }
   ],
   "source": [
    "!lspci | grep VGA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
